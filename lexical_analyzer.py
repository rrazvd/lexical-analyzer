from cursor import Cursor
from symbol_table import SymbolTable
from token2 import Token
from constants import delimiters, letter, digit, string_ascii, letter_digit_under, arithmetic_operators, relational_operators, errors_name
from constants import Tokens, Errors

""" 
This Class implements a Lexical Analyzer that recognizes tokens through finite automata. 
"""


class LexicalAnalyzer():

    def __init__(self, code, symbol_table):
        self.code = code  # input code array per line
        self.line_counter = 0  # line counter
        self.tokens, self.errors = [], []  # create token list and error token list
        self.is_open_comment = False  # boolean to know if there is an open comment
        self.comment_token = None  # used for store the comment token
        self.cursor = Cursor()  # create a cursor that will move inside the code
        self.symbol_table = symbol_table  # adds reserved words in symbol table
        self.start_analyze()  # then the analyze begins

    """ 
    This function analyze each code line and get tokens.
    """

    def start_analyze(self):

        # iterate between lines indexes
        while self.line_counter < len(self.code):
            line = self.code[self.line_counter]  # get atual line

            if (not self.is_open_comment):  # checks if not have an open comment

                while self.cursor.get_position() < len(line):  # iterate between characters indexes
                    # save position L x C
                    pos = (self.line_counter, self.cursor.get_position())
                    char = line[pos[1]]  # get atual character

                    if (letter.match(char)):  # identifiers or words
                        token = self.analyze_id_or_word(line, pos)
                    elif (digit.match(char)):  # numbers
                        token = self.analyze_numbers(line, pos)
                    elif (char == '\"'):  # strings
                        token = self.analyze_string(line, pos)
                    elif (delimiters.match(char)):  # delimiters
                        token = Token(Tokens.DELIMITER.value, char, pos)
                    elif (char == '*'):  # * operator
                        token = Token(Tokens.OP_ARITHMETIC.value, char, pos)
                    # + or ++, - or --, = or == operators
                    elif (char == '+' or char == '-' or char == '='):
                        token = self.analyze_next_char(line, char, pos)
                    # > or >=, < or <=, ! or != operators
                    elif (char == '>' or char == '<' or char == '!'):
                        token = self.analyze_next_char(line, '=', pos)
                    elif (char == '&' or char == '|'):  # && or || operator
                        token = self.analyze_logical_operator(line, char, pos)
                    elif (char == '/'):  # / operator or comments delimiters
                        token = self.analyze_divisor_operator_or_comment(
                            line, pos)
                    elif (char == ' ' or char == '\t' or char == '\n'):  # space, tab and line break
                        token = None  # no token is generated by these characters
                    else:  # invalid symbols
                        token = Token(Tokens.INVALID_SYMBOL.value, char, pos)

                    self.add_token(token)  # add resulting token
                    self.cursor.forward()  # moves the cursor forward

                self.line_counter += 1  # go to next line
                self.cursor.to_start()  # set cursor to start of line

            else:
                # search for the end of block comment
                self.find_end_block_comment(line)

                if(self.is_open_comment):  # if still open, keep search in next lines
                    self.line_counter += 1
                    self.cursor.to_start()
                else:
                    self.cursor.forward()  # else, moves cursor forward to continue reading the rest of line

        # if at the end of code there is an open comment, thus add it to the token list
        self.add_token(self.comment_token)

    """ 
    This function looks for the end of block comment: '*/',
    if it finds, set is_open_comment to True, else set is_open_comment to False.
    """

    def find_end_block_comment(self, line):
        pos = (self.line_counter, self.cursor.get_position())
        lexeme = ''  # set first lexeme character
        while self.cursor.get_position() < len(line):
            char = line[self.cursor.get_position()]
            if (char == '*'):  # if atual char is '*'
                try:
                    # get next char
                    look_ahead_char = line[self.cursor.get_look_ahead()]
                    if (look_ahead_char == '/'):  # if the next char is / so we have */
                        self.is_open_comment = False  # the end of block comment was found, so set to False
                        self.cursor.forward()  # moves the cursor forward
                        return None
                    else:
                        self.is_open_comment = True  # the end of block comment was not found, so set to True
                except (IndexError):
                    self.is_open_comment = True
            lexeme += char  # increases lexeme
            self.cursor.forward()  # moves the cursor forward
        # if the comment was not been closed, return malformed comment
        return Token(Errors.MF_COMMENT.value, lexeme, pos)

    """
    This funcion analyze identifiers or reserved word lexemes.
    """

    def analyze_id_or_word(self, line, pos):
        lexeme = line[pos[1]]  # set first lexeme character
        while self.cursor.get_look_ahead() < len(line):
            look_ahead_char = line[self.cursor.get_look_ahead()]
            if (letter_digit_under.match(look_ahead_char)):  # if the next char is letter, digit or _
                lexeme += look_ahead_char  # so increases lexeme
            else:
                # fetch token from symbol table
                return self.symbol_table.get_token(lexeme, pos)
            self.cursor.forward()  # moves the cursor forward

        # fetch token from symbol table
        return self.symbol_table.get_token(lexeme, pos)

    """
    This funcion analyze numbers lexemes.
    """

    def analyze_numbers(self, line, pos):
        lexeme = line[pos[1]]  # set first lexeme character
        self.cursor.forward()  # moves the cursor forward
        while self.cursor.get_position() < len(line):
            char = line[self.cursor.get_position()]  # get atual character
            if (digit.match(char)):  # if it is a digit
                lexeme += char  # increases lexeme
            elif (char == '.'):  # if it is a dot
                lexeme += char  # increases lexeme
                try:
                    # get next character
                    look_ahead_char = line[self.cursor.get_look_ahead()]
                    if (digit.match(look_ahead_char)):  # if it is a digit
                        self.cursor.forward()   # moves the cursor forward
                        while self.cursor.get_position() < len(line):  # iterar through next characters
                            # get atual character
                            char = line[self.cursor.get_position()]
                            if (digit.match(char)):  # if it is a digit
                                lexeme += char  # increases lexeme
                            elif (char == '.'):  # elif it is other dot
                                lexeme += char  # increases lexeme
                                # return a malformed number
                                return Token(Errors.MF_NUMBER.value, lexeme, pos)
                            else:
                                self.cursor.backward()  # moves the cursor backward
                                # return a token number
                                return Token(Tokens.NUMBER.value, lexeme, pos)
                            self.cursor.forward()  # moves the cursor forward
                    else:
                        # return a malformed number
                        return Token(Errors.MF_NUMBER.value, lexeme, pos)
                except(IndexError):
                    # return a malformed number
                    return Token(Errors.MF_NUMBER.value, lexeme, pos)
            else:
                self.cursor.backward()  # moves the cursor backward
                # return a token number
                return Token(Tokens.NUMBER.value, lexeme, pos)
            self.cursor.forward()  # moves the cursor forward
        return Token(Tokens.NUMBER.value, lexeme, pos)  # return a token number

    """
    This function analyze strings lexemes.
    """

    def analyze_string(self, line, pos):
        lexeme = line[pos[1]]  # set first lexeme character
        self.cursor.forward()
        while self.cursor.get_position() < len(line):
            char = line[self.cursor.get_position()]
            if (char == '\\'):  # if find \
                try:
                    look_ahead_char = line[self.cursor.get_look_ahead()]
                    if (look_ahead_char == '\"'):  # check if \"
                        lexeme += look_ahead_char
                        self.cursor.forward()
                    elif (look_ahead_char == '\\'):  # check if \\
                        lexeme += char
                        self.cursor.forward()
                except(IndexError):
                    return Token(Errors.MF_STRING.value, lexeme, pos)
            elif (string_ascii.match(char)):  # check allowed ascii character
                lexeme += char
            elif (char == '\"'):  # check end of string
                lexeme += char
                return Token(Tokens.STRING.value, lexeme, pos)
            else:
                return Token(Errors.MF_STRING.value, lexeme, pos)
            self.cursor.forward()
        return Token(Errors.MF_STRING.value, lexeme, pos)

    """
    This function analyze arithmetic, relational and '!' operators.
    """

    def analyze_next_char(self, line, target_char, pos):
        lexeme = line[pos[1]]  # set first lexeme character
        try:
            # get next char
            look_ahead_char = line[self.cursor.get_look_ahead()]
            if (look_ahead_char == target_char):  # if the next char is the same
                self.cursor.forward()  # move cursor forward
                lexeme += look_ahead_char  # increses lexeme
                # return corresponding token
                return Token(self.get_operator_type(lexeme), lexeme, pos)
            else:
                # return corresponding token
                return Token(self.get_operator_type(lexeme), lexeme, pos)
        except(IndexError):
            # return corresponding token
            return Token(self.get_operator_type(lexeme), lexeme, pos)

    """
    This function check the type of operators.
    """

    def get_operator_type(self, lexeme):
        if (lexeme == '!'):
            return Tokens.OP_LOGICAL.value
        elif (arithmetic_operators.match(lexeme)):
            return Tokens.OP_ARITHMETIC.value
        elif (relational_operators.match(lexeme)):
            return Tokens.OP_RELATIONAL.value

    """
    This function analyzes if the lexeme found is logical operator.
    """

    def analyze_logical_operator(self, line, target_char, pos):
        lexeme = line[pos[1]]  # set first lexeme character
        try:
            look_ahead_char = line[self.cursor.get_look_ahead()]
            if (look_ahead_char == target_char):  # if the next char is the same, so it is && or ||
                self.cursor.forward()
                lexeme += look_ahead_char
                return Token(Tokens.OP_LOGICAL.value, lexeme, pos)
            else:  # else it is a malformed operator
                return Token(Errors.MF_OPERATOR.value, lexeme, pos)
        except(IndexError):
            return Token(Errors.MF_OPERATOR.value, lexeme, pos)

    """
    This function analyzes if the lexeme found is a division operator or comment symbol.
    """

    def analyze_divisor_operator_or_comment(self, line, pos):
        lexeme = line[pos[1]]  # set first lexeme character
        try:
            look_ahead_char = line[self.cursor.get_look_ahead()]
            if (look_ahead_char == '/'):  # if //
                self.cursor.set_position(len(line))  # ignores rest of the line
                return None
            elif (look_ahead_char == '*'):  # if /*
                self.comment_token = self.find_end_block_comment(
                    line)  # search for end block comment
                return None
            else:  # else is / operator
                # return an arithmetic operator token
                return Token(Tokens.OP_ARITHMETIC.value, lexeme, pos)
        except:
            # return an arithmetic operator token
            return Token(Tokens.OP_ARITHMETIC.value, lexeme, pos)

    """
    This method adds a new token to the token list.
    """

    def add_token(self, token):
        if(token != None):
            if(token.get_name() in errors_name):
                self.errors.append(token)  # add to token error list
            else:
                self.tokens.append(token)  # add to token list

    """
    This method returns token list and token error list.
    """

    def get_tokens(self):
        return (self.tokens, self.errors)
